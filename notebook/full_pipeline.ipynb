{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft8Gr5S1Bwa5"
      },
      "source": [
        "Complete Medical Image Analysis Pipeline\n",
        "========================================\n",
        "\n",
        "A unified pipeline for knee CT scan analysis implementing:\n",
        "1. Bone Segmentation (femur and tibia)\n",
        "2. Mask Expansion (2mm and 4mm)\n",
        "3. Randomized Contour Generation\n",
        "4. Tibial Landmark Detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoHjD6xlBmmd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import SimpleITK as sitk\n",
        "from skimage import morphology, measure\n",
        "from scipy.ndimage import binary_dilation, distance_transform_edt\n",
        "import random\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WGSa-YCB62"
      },
      "source": [
        "# CONFIGURATION PARAMETERS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMtQyHBNB7mX"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration parameters for the entire pipeline\"\"\"\n",
        "\n",
        "    # Input/Output paths\n",
        "    INPUT_CT_PATH = \"../left_knee.nii.gz\"\n",
        "    RESULTS_DIR = \"results\"\n",
        "\n",
        "    # Segmentation parameters\n",
        "    BONE_THRESHOLD_HU = 300  # Hounsfield Units for bone detection\n",
        "    MIN_OBJECT_SIZE = 1000   # Minimum voxels for noise removal\n",
        "    MIN_HOLE_SIZE = 1000     # Minimum hole size to fill\n",
        "    NUM_LARGEST_COMPONENTS = 2  # Keep femur and tibia\n",
        "\n",
        "    # Expansion parameters\n",
        "    EXPANSION_2MM = 2.0      # 2mm expansion distance\n",
        "    EXPANSION_4MM = 4.0      # 4mm expansion distance\n",
        "    MAX_RANDOMIZATION_MM = 2.0  # Maximum randomization distance\n",
        "\n",
        "    # Randomization parameters\n",
        "    RANDOM_SEEDS = [42, 43]  # Seeds for reproducible randomization\n",
        "\n",
        "    # Output file names\n",
        "    ORIGINAL_MASK = \"original_mask.nii.gz\"\n",
        "    EXPANDED_2MM_MASK = \"expanded_mask_2mm.nii.gz\"\n",
        "    EXPANDED_4MM_MASK = \"expanded_mask_4mm.nii.gz\"\n",
        "    RANDOMIZED_MASK_1 = \"randomized_mask_1_seed42.nii.gz\"\n",
        "    RANDOMIZED_MASK_2 = \"randomized_mask_2_seed43.nii.gz\"\n",
        "    LANDMARKS_OUTPUT = \"tibial_landmarks.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbO6A5lHCO_K"
      },
      "source": [
        "# UTILITY FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkvNr2odCLlq"
      },
      "outputs": [],
      "source": [
        "def setup_directories():\n",
        "    \"\"\"Create necessary directories for output\"\"\"\n",
        "    os.makedirs(Config.RESULTS_DIR, exist_ok=True)\n",
        "    print(f\"✓ Results directory created: {Config.RESULTS_DIR}\")\n",
        "\n",
        "def log_step(step_name, details=\"\"):\n",
        "    \"\"\"Log pipeline steps with timestamp\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {step_name}\")\n",
        "    if details:\n",
        "        print(f\"    {details}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbNMd1FRCVWx"
      },
      "source": [
        "# TASK 1: BONE SEGMENTATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJYoUu_FCRSf"
      },
      "outputs": [],
      "source": [
        "def load_nii(path):\n",
        "    \"\"\"Load NIfTI image and return data, affine, and header\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Input file not found: {path}\")\n",
        "\n",
        "    img = nib.load(path)\n",
        "    return img.get_fdata(), img.affine, img.header\n",
        "\n",
        "def save_nii(data, affine, header, path):\n",
        "    \"\"\"Save data as NIfTI file\"\"\"\n",
        "    nib.save(nib.Nifti1Image(data.astype(np.uint8), affine, header), path)\n",
        "    print(f\"    Saved: {path}\")\n",
        "\n",
        "def segment_bone(ct_volume, threshold=Config.BONE_THRESHOLD_HU):\n",
        "    \"\"\"Segment bone based on intensity thresholding\"\"\"\n",
        "    # Apply threshold\n",
        "    bone_mask = ct_volume > threshold\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    bone_mask = morphology.remove_small_objects(\n",
        "        bone_mask.astype(bool),\n",
        "        min_size=Config.MIN_OBJECT_SIZE\n",
        "    )\n",
        "\n",
        "    # Fill small holes\n",
        "    bone_mask = morphology.remove_small_holes(\n",
        "        bone_mask,\n",
        "        area_threshold=Config.MIN_HOLE_SIZE\n",
        "    )\n",
        "\n",
        "    return bone_mask.astype(np.uint8)\n",
        "\n",
        "def keep_largest_components(mask, num_components=Config.NUM_LARGEST_COMPONENTS):\n",
        "    \"\"\"Keep the N largest connected components (femur and tibia)\"\"\"\n",
        "    labeled = measure.label(mask)\n",
        "    props = measure.regionprops(labeled)\n",
        "    props = sorted(props, key=lambda x: x.area, reverse=True)\n",
        "\n",
        "    output_mask = np.zeros_like(mask)\n",
        "    for i in range(min(num_components, len(props))):\n",
        "        output_mask[labeled == props[i].label] = 1\n",
        "\n",
        "    return output_mask\n",
        "\n",
        "def task1_bone_segmentation(ct_volume, affine, header):\n",
        "    \"\"\"Complete bone segmentation pipeline\"\"\"\n",
        "    log_step(\"TASK 1: Bone Segmentation\", \"Segmenting femur and tibia\")\n",
        "\n",
        "    # Segment bone tissue\n",
        "    raw_bone_mask = segment_bone(ct_volume)\n",
        "    print(f\"    Raw bone voxels: {np.sum(raw_bone_mask)}\")\n",
        "\n",
        "    # Keep only femur and tibia\n",
        "    final_bone_mask = keep_largest_components(raw_bone_mask)\n",
        "    print(f\"    Final bone voxels: {np.sum(final_bone_mask)}\")\n",
        "\n",
        "    # Save original mask\n",
        "    output_path = os.path.join(Config.RESULTS_DIR, Config.ORIGINAL_MASK)\n",
        "    save_nii(final_bone_mask, affine, header, output_path)\n",
        "\n",
        "    return final_bone_mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xc1mbIPCb_s"
      },
      "source": [
        "# TASK 2: MASK EXPANSION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKFtaEO7CeX3"
      },
      "outputs": [],
      "source": [
        "def expand_mask(mask_array, spacing, expansion_mm):\n",
        "    \"\"\"Expand mask uniformly by specified distance in mm\"\"\"\n",
        "    # Convert mm to voxels (spacing is x,y,z; array is z,y,x)\n",
        "    expansion_voxels = [\n",
        "        max(1, int(np.ceil(expansion_mm / spacing[2]))),  # z direction\n",
        "        max(1, int(np.ceil(expansion_mm / spacing[1]))),  # y direction\n",
        "        max(1, int(np.ceil(expansion_mm / spacing[0])))   # x direction\n",
        "    ]\n",
        "\n",
        "    print(f\"    {expansion_mm}mm → voxels: {expansion_voxels}\")\n",
        "\n",
        "    # Create 3D structuring element\n",
        "    struct_shape = tuple(2 * ev + 1 for ev in expansion_voxels)\n",
        "    structuring_element = np.ones(struct_shape, dtype=np.uint8)\n",
        "\n",
        "    # Perform binary dilation\n",
        "    expanded = binary_dilation(mask_array, structure=structuring_element)\n",
        "    return expanded.astype(np.uint8)\n",
        "\n",
        "def task2_mask_expansion(original_mask, spacing, affine, header):\n",
        "    \"\"\"Complete mask expansion for 2mm and 4mm\"\"\"\n",
        "    log_step(\"TASK 2: Mask Expansion\", \"Creating 2mm and 4mm expanded masks\")\n",
        "\n",
        "    # 2mm expansion\n",
        "    expanded_2mm = expand_mask(original_mask, spacing, Config.EXPANSION_2MM)\n",
        "    output_path_2mm = os.path.join(Config.RESULTS_DIR, Config.EXPANDED_2MM_MASK)\n",
        "    save_nii(expanded_2mm, affine, header, output_path_2mm)\n",
        "    print(f\"    2mm expanded voxels: {np.sum(expanded_2mm)}\")\n",
        "\n",
        "    # 4mm expansion\n",
        "    expanded_4mm = expand_mask(original_mask, spacing, Config.EXPANSION_4MM)\n",
        "    output_path_4mm = os.path.join(Config.RESULTS_DIR, Config.EXPANDED_4MM_MASK)\n",
        "    save_nii(expanded_4mm, affine, header, output_path_4mm)\n",
        "    print(f\"    4mm expanded voxels: {np.sum(expanded_4mm)}\")\n",
        "\n",
        "    return expanded_2mm, expanded_4mm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJe1MncIChdy"
      },
      "source": [
        "# TASK 3: RANDOMIZED CONTOUR GENERATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k2hUfdHCg4k"
      },
      "outputs": [],
      "source": [
        "def create_randomized_contour(original_mask, max_expanded_mask, spacing,\n",
        "                            random_seed, max_expansion_mm):\n",
        "    \"\"\"Create randomized contour between original and expanded boundaries\"\"\"\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # Find expansion zone\n",
        "    expansion_zone = np.logical_and(max_expanded_mask,\n",
        "                                  np.logical_not(original_mask))\n",
        "\n",
        "    # Calculate distance from original boundary\n",
        "    distance_from_original = distance_transform_edt(\n",
        "        np.logical_not(original_mask),\n",
        "        sampling=[spacing[2], spacing[1], spacing[0]]\n",
        "    )\n",
        "\n",
        "    # Start with original mask\n",
        "    randomized_mask = np.copy(original_mask)\n",
        "\n",
        "    # Process expansion zone\n",
        "    expansion_indices = np.where(expansion_zone)\n",
        "\n",
        "    for i in range(len(expansion_indices[0])):\n",
        "        z, y, x = expansion_indices[0][i], expansion_indices[1][i], expansion_indices[2][i]\n",
        "        dist = distance_from_original[z, y, x]\n",
        "\n",
        "        if dist <= max_expansion_mm:\n",
        "            # Distance-based probability\n",
        "            probability = 1 - (dist / max_expansion_mm) ** 1.5\n",
        "            probability = max(0.0, min(1.0, probability))\n",
        "\n",
        "            if random.random() < probability:\n",
        "                randomized_mask[z, y, x] = 1\n",
        "\n",
        "    return randomized_mask\n",
        "\n",
        "def validate_randomized_mask(original, expanded, randomized, max_exp_mm, spacing, seed):\n",
        "    \"\"\"Validate randomized mask meets all constraints\"\"\"\n",
        "    violations = []\n",
        "\n",
        "    # Check containment constraints\n",
        "    if not np.all((original == 1) <= (randomized == 1)):\n",
        "        violations.append(\"Original mask not fully contained\")\n",
        "\n",
        "    if not np.all((randomized == 1) <= (expanded == 1)):\n",
        "        violations.append(\"Randomized mask exceeds expansion boundary\")\n",
        "\n",
        "    # Check size constraints\n",
        "    if np.sum(original) > np.sum(randomized):\n",
        "        violations.append(\"Randomized mask smaller than original\")\n",
        "\n",
        "    if np.sum(randomized) > np.sum(expanded):\n",
        "        violations.append(\"Randomized mask larger than expanded\")\n",
        "\n",
        "    # Check distance constraint\n",
        "    distance_from_original = distance_transform_edt(\n",
        "        np.logical_not(original),\n",
        "        sampling=[spacing[2], spacing[1], spacing[0]]\n",
        "    )\n",
        "    randomized_only = np.logical_and(randomized, np.logical_not(original))\n",
        "\n",
        "    if np.any(randomized_only):\n",
        "        max_distance = np.max(distance_from_original[randomized_only])\n",
        "        if max_distance > max_exp_mm + 0.01:\n",
        "            violations.append(f\"Distance constraint violated: {max_distance:.2f}mm\")\n",
        "\n",
        "    # Report results\n",
        "    if violations:\n",
        "        print(f\"    ⚠️  Validation issues for seed {seed}:\")\n",
        "        for v in violations:\n",
        "            print(f\"       - {v}\")\n",
        "    else:\n",
        "        print(f\"    ✓ Validation passed for seed {seed}\")\n",
        "        print(f\"      Voxels: {np.sum(original)} → {np.sum(randomized)} → {np.sum(expanded)}\")\n",
        "\n",
        "def task3_randomized_contours(original_mask, expanded_2mm, spacing, affine, header):\n",
        "    \"\"\"Generate randomized contours with validation\"\"\"\n",
        "    log_step(\"TASK 3: Randomized Contour Generation\",\n",
        "             f\"Creating {len(Config.RANDOM_SEEDS)} randomized versions\")\n",
        "\n",
        "    randomized_masks = []\n",
        "    filenames = [Config.RANDOMIZED_MASK_1, Config.RANDOMIZED_MASK_2]\n",
        "\n",
        "    for i, seed in enumerate(Config.RANDOM_SEEDS):\n",
        "        print(f\"    Generating randomized mask {i+1} (seed {seed})\")\n",
        "\n",
        "        # Create randomized contour\n",
        "        randomized = create_randomized_contour(\n",
        "            original_mask, expanded_2mm, spacing, seed, Config.MAX_RANDOMIZATION_MM\n",
        "        )\n",
        "\n",
        "        # Save mask\n",
        "        output_path = os.path.join(Config.RESULTS_DIR, filenames[i])\n",
        "        save_nii(randomized, affine, header, output_path)\n",
        "\n",
        "        # Validate\n",
        "        validate_randomized_mask(\n",
        "            original_mask, expanded_2mm, randomized,\n",
        "            Config.MAX_RANDOMIZATION_MM, spacing, seed\n",
        "        )\n",
        "\n",
        "        randomized_masks.append(randomized)\n",
        "\n",
        "    return randomized_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVDkITJ8Cogf"
      },
      "source": [
        "# TASK 4: TIBIAL LANDMARK DETECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NAJawZXCmVQ"
      },
      "outputs": [],
      "source": [
        "def find_tibia_from_mask(mask_array):\n",
        "    \"\"\"Extract tibia (lower bone) from mask\"\"\"\n",
        "    # Convert to SimpleITK for connected component analysis\n",
        "    mask_sitk = sitk.GetImageFromArray(mask_array)\n",
        "    labeled_sitk = sitk.ConnectedComponent(mask_sitk)\n",
        "    labeled_array = sitk.GetArrayFromImage(labeled_sitk)\n",
        "\n",
        "    # Get component statistics\n",
        "    stats = sitk.LabelShapeStatisticsImageFilter()\n",
        "    stats.Execute(labeled_sitk)\n",
        "\n",
        "    # Find component with lowest centroid (tibia)\n",
        "    min_z = float('inf')\n",
        "    tibia_label = None\n",
        "\n",
        "    for label in stats.GetLabels():\n",
        "        centroid = stats.GetCentroid(label)\n",
        "        if centroid[2] < min_z:  # z-coordinate (superior-inferior)\n",
        "            min_z = centroid[2]\n",
        "            tibia_label = label\n",
        "\n",
        "    # Create tibia-only mask\n",
        "    tibia_mask = (labeled_array == tibia_label).astype(np.uint8)\n",
        "    return tibia_mask\n",
        "\n",
        "def find_tibial_landmarks(tibia_mask, spacing):\n",
        "    \"\"\"Find medial and lateral lowest points on tibial surface\"\"\"\n",
        "    # Find lowest slice with bone\n",
        "    z_slices = np.any(tibia_mask, axis=(1, 2))\n",
        "    if not np.any(z_slices):\n",
        "        return None, None\n",
        "\n",
        "    lowest_z = np.where(z_slices)[0][-1]\n",
        "    lowest_slice = tibia_mask[lowest_z]\n",
        "\n",
        "    # Find boundary of bone in lowest slice\n",
        "    boundary_sitk = sitk.GetImageFromArray(lowest_slice.astype(np.uint8))\n",
        "    boundary_sitk = sitk.BinaryContour(boundary_sitk)\n",
        "    boundary_array = sitk.GetArrayFromImage(boundary_sitk)\n",
        "\n",
        "    # Get boundary points\n",
        "    boundary_points = np.where(boundary_array)\n",
        "    if len(boundary_points[0]) == 0:\n",
        "        return None, None\n",
        "\n",
        "    y_coords = boundary_points[0]\n",
        "    x_coords = boundary_points[1]\n",
        "\n",
        "    # Find leftmost (medial) and rightmost (lateral) points\n",
        "    left_idx = np.argmin(x_coords)\n",
        "    right_idx = np.argmax(x_coords)\n",
        "\n",
        "    # Convert to physical coordinates (mm)\n",
        "    medial_point = [\n",
        "        float(x_coords[left_idx] * spacing[0]),   # x\n",
        "        float(y_coords[left_idx] * spacing[1]),   # y\n",
        "        float(lowest_z * spacing[2])              # z\n",
        "    ]\n",
        "\n",
        "    lateral_point = [\n",
        "        float(x_coords[right_idx] * spacing[0]),  # x\n",
        "        float(y_coords[right_idx] * spacing[1]),  # y\n",
        "        float(lowest_z * spacing[2])              # z\n",
        "    ]\n",
        "\n",
        "    return medial_point, lateral_point\n",
        "\n",
        "def task4_landmark_detection(all_masks, spacing):\n",
        "    \"\"\"Detect tibial landmarks for all mask types\"\"\"\n",
        "    log_step(\"TASK 4: Tibial Landmark Detection\", \"Processing all 5 masks\")\n",
        "\n",
        "    mask_names = [\n",
        "        \"original\",\n",
        "        \"expanded_2mm\",\n",
        "        \"expanded_4mm\",\n",
        "        \"randomized_1\",\n",
        "        \"randomized_2\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for i, (mask_name, mask_array) in enumerate(zip(mask_names, all_masks)):\n",
        "        print(f\"    Processing {mask_name} mask...\")\n",
        "\n",
        "        # Extract tibia\n",
        "        tibia_mask = find_tibia_from_mask(mask_array)\n",
        "\n",
        "        # Find landmarks\n",
        "        medial_point, lateral_point = find_tibial_landmarks(tibia_mask, spacing)\n",
        "\n",
        "        if medial_point is not None and lateral_point is not None:\n",
        "            results[mask_name] = {\n",
        "                \"medial_point\": medial_point,\n",
        "                \"lateral_point\": lateral_point,\n",
        "                \"tibia_voxels\": int(np.sum(tibia_mask))\n",
        "            }\n",
        "            print(f\"      Medial:  [{medial_point[0]:.2f}, {medial_point[1]:.2f}, {medial_point[2]:.2f}]\")\n",
        "            print(f\"      Lateral: [{lateral_point[0]:.2f}, {lateral_point[1]:.2f}, {lateral_point[2]:.2f}]\")\n",
        "        else:\n",
        "            print(f\"      ⚠️  Could not detect landmarks for {mask_name}\")\n",
        "            results[mask_name] = {\n",
        "                \"medial_point\": None,\n",
        "                \"lateral_point\": None,\n",
        "                \"tibia_voxels\": 0,\n",
        "                \"error\": \"Landmark detection failed\"\n",
        "            }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zXoYGDvCy_3"
      },
      "source": [
        "# MAIN PIPELINE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsbXORWVCwjI",
        "outputId": "101e7cf2-22b5-4a2f-f86c-7bedcfca86ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MEDICAL IMAGE ANALYSIS PIPELINE\n",
            "======================================================================\n",
            "Input: /content/left_knee.nii.gz\n",
            "Output: results/\n",
            "======================================================================\n",
            "✓ Results directory created: results\n",
            "[17:31:51] LOADING DATA\n",
            "    Reading CT scan: /content/left_knee.nii.gz\n",
            "    Image shape: (512, 512, 216)\n",
            "    Voxel spacing: (0.8691409826278687, 0.8691409826278687, 2.0) mm\n",
            "    Intensity range: [-3024.0, 1769.0] HU\n",
            "[17:31:55] TASK 1: Bone Segmentation\n",
            "    Segmenting femur and tibia\n",
            "    Raw bone voxels: 288032\n",
            "    Final bone voxels: 269826\n",
            "    Saved: results/original_mask.nii.gz\n",
            "[17:32:06] TASK 2: Mask Expansion\n",
            "    Creating 2mm and 4mm expanded masks\n",
            "    2.0mm → voxels: [1, 3, 3]\n",
            "    Saved: results/expanded_mask_2mm.nii.gz\n",
            "    2mm expanded voxels: 612786\n",
            "    4.0mm → voxels: [2, 5, 5]\n",
            "    Saved: results/expanded_mask_4mm.nii.gz\n",
            "    4mm expanded voxels: 805907\n",
            "[17:33:01] TASK 3: Randomized Contour Generation\n",
            "    Creating 2 randomized versions\n",
            "    Generating randomized mask 1 (seed 42)\n",
            "    Saved: results/randomized_mask_1_seed42.nii.gz\n",
            "    ✓ Validation passed for seed 42\n",
            "      Voxels: 269826 → 353666 → 612786\n",
            "    Generating randomized mask 2 (seed 43)\n",
            "    Saved: results/randomized_mask_2_seed43.nii.gz\n",
            "    ✓ Validation passed for seed 43\n",
            "      Voxels: 269826 → 354008 → 612786\n",
            "[17:34:15] TASK 4: Tibial Landmark Detection\n",
            "    Processing all 5 masks\n",
            "    Processing original mask...\n",
            "      Medial:  [80.83, 244.23, 854.00]\n",
            "      Lateral: [84.31, 239.01, 854.00]\n",
            "    Processing expanded_2mm mask...\n",
            "      Medial:  [70.40, 258.13, 884.00]\n",
            "      Lateral: [75.62, 258.13, 884.00]\n",
            "    Processing expanded_4mm mask...\n",
            "      Medial:  [68.66, 256.40, 886.00]\n",
            "      Lateral: [77.35, 256.40, 886.00]\n",
            "    Processing randomized_1 mask...\n",
            "      Medial:  [109.51, 237.28, 646.00]\n",
            "      Lateral: [109.51, 237.28, 646.00]\n",
            "    Processing randomized_2 mask...\n",
            "      Medial:  [98.21, 238.14, 646.00]\n",
            "      Lateral: [98.21, 238.14, 646.00]\n",
            "[17:34:20] SAVING RESULTS\n",
            "    Writing landmark coordinates\n",
            "    Saved: results/tibial_landmarks.json\n",
            "[17:34:20] PIPELINE COMPLETE\n",
            "    All tasks finished successfully\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "Generated Files:\n",
            "  ✓ original_mask.nii.gz\n",
            "  ✓ expanded_mask_2mm.nii.gz\n",
            "  ✓ expanded_mask_4mm.nii.gz\n",
            "  ✓ randomized_mask_1_seed42.nii.gz\n",
            "  ✓ randomized_mask_2_seed43.nii.gz\n",
            "  ✓ tibial_landmarks.json\n",
            "\n",
            "Landmark Detection Results:\n",
            "  ✓ original: Landmarks detected\n",
            "  ✓ expanded_2mm: Landmarks detected\n",
            "  ✓ expanded_4mm: Landmarks detected\n",
            "  ✓ randomized_1: Landmarks detected\n",
            "  ✓ randomized_2: Landmarks detected\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Execute the complete medical image analysis pipeline\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"MEDICAL IMAGE ANALYSIS PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Input: {Config.INPUT_CT_PATH}\")\n",
        "    print(f\"Output: {Config.RESULTS_DIR}/\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Setup\n",
        "    setup_directories()\n",
        "\n",
        "    try:\n",
        "        # Load input CT scan\n",
        "        log_step(\"LOADING DATA\", f\"Reading CT scan: {Config.INPUT_CT_PATH}\")\n",
        "        ct_volume, affine, header = load_nii(Config.INPUT_CT_PATH)\n",
        "\n",
        "        # Get spatial information\n",
        "        ct_sitk = sitk.ReadImage(Config.INPUT_CT_PATH)\n",
        "        spacing = ct_sitk.GetSpacing()  # (x, y, z)\n",
        "        print(f\"    Image shape: {ct_volume.shape}\")\n",
        "        print(f\"    Voxel spacing: {spacing} mm\")\n",
        "        print(f\"    Intensity range: [{ct_volume.min():.1f}, {ct_volume.max():.1f}] HU\")\n",
        "\n",
        "        # Task 1: Bone Segmentation\n",
        "        original_mask = task1_bone_segmentation(ct_volume, affine, header)\n",
        "\n",
        "        # Task 2: Mask Expansion\n",
        "        expanded_2mm, expanded_4mm = task2_mask_expansion(\n",
        "            original_mask, spacing, affine, header\n",
        "        )\n",
        "\n",
        "        # Task 3: Randomized Contours\n",
        "        randomized_masks = task3_randomized_contours(\n",
        "            original_mask, expanded_2mm, spacing, affine, header\n",
        "        )\n",
        "\n",
        "        # Prepare all masks for landmark detection\n",
        "        all_masks = [\n",
        "            original_mask,\n",
        "            expanded_2mm,\n",
        "            expanded_4mm,\n",
        "            randomized_masks[0],\n",
        "            randomized_masks[1]\n",
        "        ]\n",
        "\n",
        "        # Task 4: Landmark Detection\n",
        "        landmark_results = task4_landmark_detection(all_masks, spacing)\n",
        "\n",
        "        # Save landmark results\n",
        "        log_step(\"SAVING RESULTS\", \"Writing landmark coordinates\")\n",
        "        results_path = os.path.join(Config.RESULTS_DIR, Config.LANDMARKS_OUTPUT)\n",
        "\n",
        "        # Add metadata to results\n",
        "        final_results = {\n",
        "            \"metadata\": {\n",
        "                \"input_file\": Config.INPUT_CT_PATH,\n",
        "                \"processing_timestamp\": datetime.now().isoformat(),\n",
        "                \"voxel_spacing_mm\": list(spacing),\n",
        "                \"parameters\": {\n",
        "                    \"bone_threshold_hu\": Config.BONE_THRESHOLD_HU,\n",
        "                    \"expansion_2mm\": Config.EXPANSION_2MM,\n",
        "                    \"expansion_4mm\": Config.EXPANSION_4MM,\n",
        "                    \"randomization_seeds\": Config.RANDOM_SEEDS,\n",
        "                    \"max_randomization_mm\": Config.MAX_RANDOMIZATION_MM\n",
        "                }\n",
        "            },\n",
        "            \"landmarks\": landmark_results\n",
        "        }\n",
        "\n",
        "        with open(results_path, 'w') as f:\n",
        "            json.dump(final_results, f, indent=2)\n",
        "        print(f\"    Saved: {results_path}\")\n",
        "\n",
        "        # Summary\n",
        "        log_step(\"PIPELINE COMPLETE\", \"All tasks finished successfully\")\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"SUMMARY\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"Generated Files:\")\n",
        "        for filename in [Config.ORIGINAL_MASK, Config.EXPANDED_2MM_MASK,\n",
        "                        Config.EXPANDED_4MM_MASK, Config.RANDOMIZED_MASK_1,\n",
        "                        Config.RANDOMIZED_MASK_2, Config.LANDMARKS_OUTPUT]:\n",
        "            filepath = os.path.join(Config.RESULTS_DIR, filename)\n",
        "            if os.path.exists(filepath):\n",
        "                print(f\"  ✓ {filename}\")\n",
        "            else:\n",
        "                print(f\"  ✗ {filename} (MISSING)\")\n",
        "\n",
        "        print(f\"\\nLandmark Detection Results:\")\n",
        "        for mask_name, result in landmark_results.items():\n",
        "            if result.get(\"medial_point\") is not None:\n",
        "                print(f\"  ✓ {mask_name}: Landmarks detected\")\n",
        "            else:\n",
        "                print(f\"  ✗ {mask_name}: {result.get('error', 'Failed')}\")\n",
        "\n",
        "        print(\"=\"*70)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"❌ ERROR: {e}\")\n",
        "        print(\"Please ensure the input CT file exists at the specified path.\")\n",
        "        return 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PIPELINE ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit_code = main()\n",
        "    exit(exit_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU51JVm1DHVy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
